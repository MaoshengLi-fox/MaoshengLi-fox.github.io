---
title: 'dimensionality reduction algorithm'
date: 2025-08-25
permalink: /posts/2025/08/blog-post-1/
tags:
  - dimensionality reduction

---

dimensionality reduction algorithm
======

[TOC]



------

#### 线性降维方法

##### 主成分分析（PCA, Principal Component Analysis）

- 最基础的无监督降维算法

- 目标：将高维数据通过线性变换映射到低维空间，尽量**保留方差/信息**（最大方差）、或**最小化重构误差**（最小误差）====> 因此有两种等价优化观点

  - 最大方差投影公式
    $$
    \max_{V_k^\top V_k = I}\ 
    \mathrm{Tr}\!\left(
    V_k^\top \,\frac{1}{n-1}\tilde X^\top \tilde X\, V_k
    \right)
    $$
    ![image-20250822163110368](C:\Users\86182\AppData\Roaming\Typora\typora-user-images\image-20250822163110368.png)

  - 最小重构误差公式
    $$
    \min_{V_k^\top V_k = I}\ 
    \left\| \tilde X - \tilde X V_k V_k^\top \right\|_F^2
    $$
    ![image-20250822163248355](C:\Users\86182\AppData\Roaming\Typora\typora-user-images\image-20250822163248355.png)

    ps：Frobenius 范数是矩阵范数，L2范数是向量范数

- 计算方法

  - SVD路线（数值最稳）Singular Value Decomposition
    - 直接对中心化后数据矩阵做奇异值分解$$\tilde X = U \sum V^T$$，主成分就是V的列
    - SVD是一种矩阵分解方法，将一个实矩阵分解为三个矩阵的乘积
    - 步骤：给定$$A\in R^{m*n}$$，计算对称矩阵$$A^TA\in R^{n*n}$$或$$AA^T\in R^{m*m}$$，在PCA中这一步计算的协方差矩阵；解特征方程$$det(A^TA-\lambda I)=0$$得到特征值，进而得到特征向量也就是V的列向量；特征值开方得到奇异值，从大到小放到$$\sum$$的对角线；计算U
    - <font color="red">**注意PCA计算时，SVD前要先中心化数据，因为协方差矩阵只关心方向，不能有均值进行贡献**</font>

- <font color="orange">**特征分解**</font>

  - 对于n x n的方阵A，如果存在非零向量v和标量$$\lambda$$，满足：$$Av=\lambda v$$，则称$$\lambda$$为特征值，v为特征向量
  - 矩阵形式为：$$A=V\Lambda V^{-1}$$，其中，V由特征向量组成，$$\Lambda$$为对角矩阵，特征值在主对角线上

- 应用：

  - 数据分析（找到主要变化方向）
  - 可视化（降到 2D/3D）
  - 数据预处理


```python
 sklearn.decomposition.PCA
```

##### <font color="orange">奇异值分解（SVD, Singular Value Decomposition）</font>

- 任意数据矩阵$$X \in R^{m*n}$$可以写成：$$X=U\Sigma V^T$$
  - U：左奇异向量（与样本相关），来自$$XX^T$$的特征向量
  - $$\Sigma$$：奇异值（数值大小表示信息量），对角线为非负实数
  - V：右奇异向量（与特征维度相关），来自$$X^TX$$的特征向量

- 应用：
  - 信息检索（Latent Semantic Analysis，文本降维）
  - 图像压缩（保留前 k 个奇异值）
  - 一般的低秩近似和噪声去除
- <font color="red">**与特征分解的关系**</font>
  - 对于任意矩阵 X 做SVD ：$$X=U\Sigma V^T$$
  - 两边右乘 V，得到：$$XV=U\Sigma $$
  - 两边左乘$$X^T$$，得到：$$X^TXV=V\Sigma^2 $$， 特征分解的形式为 $$Av=\lambda v$$
  - 对号入座：$$A = X^TX, \lambda=\Sigma^2, v= V$$
  - 也就是说SVD的右奇异向量是X的特征向量，奇异值平方等于特征值
  - <font color="orange">因此 **PCA 实际上是对数据矩阵的 SVD 在统计意义上的应用**</font>

##### 线性判别分析（LDA, Linear Discriminant Analysis）

- 一种**有监督的线性降维方法**

- 目标：找到一个低维子空间，使得样本在该子空间中 **最大化类间差异**、**最小化类内差异**，利用了**类别标签信息**

- 过程：

  - 类内散度矩阵反映同类样本的紧凑程度：$$S_W=\sum_{i=1}^c\sum_{x\in C_i}(x-\mu_i)(x-\mu_i)^T$$
  - 类间散度矩阵反映类别之间的分离程度：$$S_B=\sum_{i=1}^cN_i(\mu_i-\mu)(\mu_i-\mu)^T$$，$$N_i$$第i类样本数
  - 找到投影矩阵W，使得 $$max_W \frac{|W^TS_BW|}{|W^TS_WW|}$$，广义特征值分解问题，分子分母分别表示投影后类间散度和类内散度。max====>变大分子，变小分母

- <font color="green">ps：如果类别数为 c，那么最多能降到 c−1 维</font>

- 应用：

  - 分类任务前常用 LDA 降维，可以增强类别可分性，提高分类器（如逻辑回归、SVM）的性能
  - 也可单独作为分类器：**Fisher 线性判别分析（Fisher’s LDA）**

- ```python
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
  
  lda = LDA(n_components=2)  # 保留2维
  X_lda = lda.fit_transform(X, y)  # X是特征，y是标签
  ```

##### 因子分析（FA, Factor Analysis）

- 核心思想：
  - 假设观测到的高维数据（多个变量）可以由 **少数几个潜在的、不可直接观测的因子（latent factors）** 共同驱动，**目标是解释变量的相关结构**，解释变量之间的关系
  - 每个观测变量都由少数几个公共因子（common factors）和一个特有因子（unique factor, 也叫特殊因子或噪声项）线性组合得到。观测变量向量为$$X=(X_1,X_2,...X_p)^T$$，即：$$X=\mu+\Lambda F+\epsilon$$
    - $$\mu$$  均值向量
    - $$F=(F_1,F_2,...F_m)^T $$，其中(m<<p)，少数公共因子
    - $$\Lambda$$：（p X m）因子载荷矩阵（factor loading matrix），表示因子对观测变量的影响程度
    - $$\epsilon=(\epsilon_1,...,\epsilon_p)^T$$：独特因子（unique factors），每个变量特有的部分
    - 可以将因子分析想象成两个方片重叠，重叠部分为公共因子，独特因子为各自非重叠部分，所以$$\epsilon$$有p个，而公共因子少于p个
- 步骤
  - **相关矩阵分析**：先计算变量之间的相关矩阵，因子分析依赖相关性。
  - **因子提取**：通过特征值分解（如最大似然法、主因子法），提取若干因子。
    - 通常选取特征值 > 1 的因子（Kaiser 准则），或使用碎石图（Scree plot）。
  - **因子旋转**：旋转因子坐标轴，使得因子载荷矩阵更“稀疏”（稀疏就更能说明哪些是主要因子，哪些是无关因子），便于解释。
    - 因子模型为$$y-\mu=Lf+\epsilon$$，假设T是一个任意正交矩阵，则$$TT^T=T^TT=I$$，则因子模型等价于$$y-\mu=L^*f^*+\epsilon,L^*=LT,f^*=T^Tf$$。<font color="orange">表明因子及其载荷矩阵不唯一，可以按照任意正交矩阵T进行旋转，因此我们可以寻找使得因子以及载荷结构更简单和解释更清晰的旋转方向T</font>
    - 正交旋转（如 Varimax）保证因子之间不相关。
    - 斜交旋转（如 Promax）允许因子之间相关。
  - **因子解释**：根据因子载荷矩阵，解释每个因子代表的潜在含义。

##### 独立成分分析（ICA, Independent Component Analysis）

- 假设数据是是**若干相互独立的非高斯源信号**经**线性混合**得到的，目标是希望找到一组线性变换，将原始信号（观测信号）通过这些变换变为独立的成分。**用独立性而不是方差/协方差**定义好的低维表示。
- 应用
  - **盲源分离，**指从混合信号中恢复出原始信号的过程。eg.从混合的音频信号中分离出不同的音源，如人声、音乐、环境噪音；在脑电图（EEG）信号处理中，ICA可以用于分离出不同的脑电波成分，从而帮助研究人员分析大脑活动。
  - **信号去噪。**当信号被噪音污染时，可以将噪音视为混合信号的一个成分。ICA分析，可以将原始信号中的噪音成分与真实信号成分分离开来，从而实现信号的去噪处理。
  - **特征提取。**在图像处理中，可以将图像的像素值作为混合信号，然后使用ICA从中提取出基础成分，这些基础成分可以表示图像的特征。类似地，在语音识别中，ICA可以从声谱图中提取出语音信号的特征，有助于提高识别性能。
- 具体实现算法
  - FastICA（固定点迭代，最快、最常用）
  - Infomax（最大熵/最小互信息，梯度法）
  - JADE/SOBI（基于高阶矩或时序结构的联合对角化）

##### comparison

- PCA旨在最大化数据方差，而不考虑成分之间的独立性。相比之下，ICA通过假设成分相互独立，可以分离出原始信号的独立成分。
- 因子分析是另一种多变量分析方法，旨在揭示观测数据背后的潜在结构和模式。与ICA不同，因子分析假设观测数据是潜在因子和误差项的线性组合。因子分析更关注观测数据的共同性，而ICA更关注成分的独立性。

#### 非线性降维方法

优化损失函数实现数据降维，而不是采用线性映射的方式

##### 多维尺度分析（MDS, Multidimensional Scaling）

- 基于距离度量的数据降维方法，要求将高维数据转换为低维后，样本点间相对位置关系不变
- 即最小化$$\sum(||z_i-z_j||-d_{ij})^2$$
  - $$||z_i-z_j||$$：低维样本间的欧式距离
  - $$d_{ij}$$：高维样本$$x_i 和x_j$$之间的距离或者不相似度 （度量方式任意，如欧式距离）
- 一般步骤
  - 利用给定数据计算距离矩阵（不相似度矩阵）D
  - 计算降维后矢量z的互相关矩阵B
  - 对B进行特征值分解，选取较大的若干特征值与特征矢量获取Z

##### Isomap等距特征映射

- 是MDS算法在流形结构上的应用

- 核心思想：**用流形上的测地线距离（沿着数据流形表面走的最短路）来替代欧氏距离**，然后让低维嵌入**尽可能保留这些测地线距离**

##### 局部线性嵌入（LLE, Locally Linear Embedding）

- 核心思想：通过样本点的邻接点的线性组合近似样本点，表示为$$\sum_{j}w_{ij}x^j$$，$$w_{ij}$$表示$$x^i和x^j$$的关系，因此目标为找到一组$$w_{ij}$$使得$$\sum_{i}||x^i-\sum_{j}w_{ij}x^j||$$最小
- **当在高维空间确定权重$$w_{ij}$$后，低维空间保持权重$$w_{ij}$$不变**，目标为找到低维表示z，同样使得$$\sum_{i}||z^i-\sum_{j}w_{ij}z^j||$$最小
- ps：邻接点数目K为超参数，需要自己调试确定

##### t-SNE (t-distributed Stochastic Neighbor Embedding)

- 是SNE方法的优化版本

- 核心思想：分别在高维度空间和低维度空间定义一个**概率分布**，这两个概率分布都描述的两个样本点之间的“距离”（如欧式距离等），然后通过**最小化高维和低维概率分布之间的差异（KL散度）**来学习低维嵌入。

- |          | SNE                                                          | t-SNE                                                        |
  | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 高维     | $$p_{j|i}=\frac{exp(-||x_i-x_j||^2/2\sigma_i^2)}{\sum_{k\neq i}exp(-||x_i-x_k||^2/2\sigma_i^2)}$$ | 不仅高维，而且低维，将条件概率改为对称的联合概率<br />$$p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}$$ |
  | 低维     | $$q_{j|i}=\frac{exp(-||y_i-y_j||^2)}{\sum_{k\neq i}exp(-||y_i-y_k||^2)}$$ | 改为t分布，自由度为1：$$q_{ij}=\frac{1+||y_i-y_j||^{-1}}{\sum_{k\neq l}(1+||y_k-y_l||^2)^{-1}}$$ |
  | 优化目标 | $$C=\sum_{i}KL(P_i||Q_i)=\sum_{i}\sum_{j}p_{j|i}log{\frac{p_{j|i}}{q_{j|i}}}$$ | $$C=KL(P||Q)=\sum_{i\neq j}p_{ij}log{\frac{p_{ij}}{q_{ij}}}$$ |
  | 问题     | **梯度消失**：当点之间距离较远时，概率接近 0，优化困难<br />**拥挤问题（Crowding Problem）**：高维数据分布在低维空间难以准确表示，点容易拥挤。 | **计算复杂度高**：标准 t-SNE 时间复杂度 $$O(N^2)$$。大数据时需要近似加速（如 Barnes-Hut t-SNE, FFT-based t-SNE）。<br />**参数敏感**：perplexity（困惑度，控制有效邻居数）需要手动调节<br />**全局结构不一定保留**：聚类形状好，但簇间相对位置可能失真。 |

- ```python
  from sklearn.manifold import TSNE
  Y = TSNE(n_components=2, perplexity=30, learning_rate='auto', n_iter=1000,
           init='pca', early_exaggeration=12.0, verbose=1).fit_transform(X)
  
  ```

  | parameter          | remark                                                       |
  | ------------------ | ------------------------------------------------------------ |
  | n_components       | 目标维度（通常设为 2 或 3，用于可视化）                      |
  | perplexity         | 困惑度，控制每个点“有效邻居”的个数。一般取 5–50，太小会过拟合局部，太大会稀释邻域。样本数必须 > 3 × perplexity，否则会报错 |
  | init               | 低维空间的初始坐标。`"random"`：随机初始化，可能导致不同运行结果差别大。 `"pca"`：用 PCA 前几维初始化，更稳定、收敛更快 |
  | method             | 优化算法。`"barnes_hut"`：近似算法，复杂度 O(Nlog⁡N)，适合中等规模数据（N < 50k）。`"exact"`：精确计算，复杂度 O(N2)，只适合小数据 |
  | early_exaggeration | 前若干次迭代中将高维概率分布 P 放大该倍数。用于在初期增强“吸引力”，使得簇更快聚集。通常取 12（默认），在 250 次迭代后恢复正常 |
  | verbose            | 设置 > 0 会打印优化过程（KL 散度变化）                       |

  

##### UMAP (Uniform Manifold Approximation and Projection)

- 相比于SNE和TSNE，UMAP通过将样本构建为图，并用交叉熵使高维图和低维图的边权分布相似来优化高维数据。

- 高维图构建

  - 为高维空间每个样本（点）找出k个最邻近的样本（k ≈ n_neighbors）
  - 定义每条边的权重为：$$w_{ij} = exp(-\frac{d(x_i,x_j)-\rho_i}{\sigma_i}) $$
  - $$d(x_i,x_j)$$： 欧式距离
  - $$\rho_i$$：局部最小距离（保证每点至少有一个邻居）
  - $$\sigma_i$$：缩放因子，控制密度平衡

- 低维图构建

  - 在低维空间（通常 2D/3D）随机初始化点
  - 定义边权重

- 通过**最小化交叉熵**和**随机梯度下降(SGD)**优化低维坐标

- ```python
  reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
  emb = reducer.fit_transform(data)  # -> (N,2)
  ```

  - | parameter    | remark                             | value                                                        |
    | ------------ | ---------------------------------- | ------------------------------------------------------------ |
    | n_neighbors  | 每个点在高维空间里考虑的最近邻个数 | 小（如 5–15）局部聚类更明显；<br />大（如 50–200）大（如 50–200）整体分布更平滑 |
    | min_dist     | 在低维空间中点之间的最小距离       | 小（接近 0）：簇更紧凑，点云拥挤，适合突出聚类<br />大（接近 0.5–0.8）：点更分散，保留全局连续性 |
    | n_components | 目标降维维度（默认 2，用于可视化） |                                                              |
    | metric       | 高维空间中计算距离的度量方式       | euclidean(欧式距离),manhattan,cosine, correlation, hamming   |

  

#### 特征选择方法

##### Filter 

##### Wrapper 

##### Embedded

#### 基于深度学习的方法

#### 预先知识

- $$
  方差：Var(X)=E[(X−E[X])^2]
  $$

- $$
  协方差：Cov(X,Y) &=E[(X−E[X])(Y−E[Y])] \\
           &=E(XY)-E(X)E(Y)
  $$

  

##### 个人理解

- 矩阵乘法$$C_{m*n}=X_{m*k}Y_{k*n}$$定义$$c_{ij}=\sum_{z=0}^{k} x_{i,z}y_{z,j}$$，可以理解为左矩阵处理一行的数据，右矩阵处理一列的数据。在机器学习中，数据矩阵一般表示为m*n，m个数据，n维特征。当数据需要降维时，其目的为减少（增大也没影响）数据特征维度，因此数据矩阵会右乘降维矩阵，借此控制处理后的数据矩阵的特征维度。
- 所以，也可以将数据矩阵左乘一个矩阵，处理同一维度的数据。例如，数据矩阵左乘一个权重矩阵，借此对不同维度的数据乘上不同的权重。
- 注意<font color="red">**协方差矩阵（总体）vs样本协方差矩阵**</font>
  - 样本协方差矩阵：$$S=\frac{1}{m-1}\sum_{k=1}^{m}(x_i^{(k)}-\tilde x_i)(x_j^{(k)}-\tilde x_j)$$ ，m-1是为了得到无偏估计（Bessel校正）
- 中心化（center）：是对每一列（特征）减去它的均值，使得均值为 0
- 白化（whiten）：把每个主成分（principal component，PC）方向上的方差标准化为 1
- 载荷矩阵（loadings / components）：PCA 的特征方向矩阵$$ V_k $$的转置


##### Q:

- <font color="red">**在PCA问题中，如何将SVD与最大方差投影公式结合**</font>
