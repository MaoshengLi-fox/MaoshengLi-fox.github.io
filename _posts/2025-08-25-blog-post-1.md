---
title: "dimensionality reduction algorithm"
date: 2025-08-25 20:00:00 +0400
permalink: /posts/2025/08/blog-post-1/
tags:
  - dimensionality reduction
  - notes
---

* 目录
{:toc}



------

### 线性降维方法

#### 主成分分析（PCA, Principal Component Analysis）

- 最基础的无监督降维算法。

- 目标：将高维数据通过线性变换映射到低维空间，尽量**保留方差/信息**（最大方差）、或**最小化重构误差**（最小误差）——因此有两种等价优化观点。

  - 最大方差投影公式

    {% raw %}
    $$
    \max_{V_k^\top V_k = I}\ 
    \mathrm{Tr}\!\left(
    V_k^\top \,\frac{1}{n-1}\tilde X^\top \tilde X\, V_k
    \right)
    $$
    {% endraw %}

    ![PCA-variance]({{ site.baseurl }}/assets/img/pca-var.png)

  - 最小重构误差公式

    {% raw %}
    $$
    \min_{V_k^\top V_k = I}\ 
    \left\| \tilde X - \tilde X V_k V_k^\top \right\|_F^2
    $$
    {% endraw %}

    ![PCA-recon]({{ site.baseurl }}/assets/img/pca-recon.png)

    ps：Frobenius 范数是矩阵范数，L2 范数是向量范数。

- 计算方法

  - **SVD 路线（数值最稳）**：Singular Value Decomposition  
    直接对**中心化**后的数据矩阵做奇异值分解：

    {% raw %}
    $$
    \tilde X = U\, \Sigma\, V^\top
    $$
    {% endraw %}

    主成分就是 $V$ 的列。

    - SVD 是一种矩阵分解方法，将一个实矩阵分解为三个矩阵的乘积。
    - 步骤：给定 {% raw %}$$A\in\mathbb{R}^{m\times n}$${% endraw %}，计算 {% raw %}$$A^\top A\in\mathbb{R}^{n\times n}$${% endraw %} 或 {% raw %}$$AA^\top\in\mathbb{R}^{m\times m}$${% endraw %}（在 PCA 中这一步等价于协方差矩阵）；解特征方程 {% raw %}$$\det(A^\top A-\lambda I)=0$${% endraw %} 得到特征值与特征向量，也就是 $$V$$ 的列向量；特征值开方得到奇异值，按从大到小放到 $$\Sigma$$ 的对角线；计算 $$U$$。
    - **注意**：PCA 计算时，**在 SVD 前要先对数据做中心化**，因为协方差矩阵只关心方向，不能让均值贡献方差。

- **特征分解**

  - 对于 {% raw %}$$n\times n$${% endraw %} 的方阵 $$A$$，若存在非零向量 $$v$$ 和标量 {% raw %}$$\lambda$${% endraw %}，满足 {% raw %}$$Av=\lambda v$${% endraw %}，则称 {% raw %}$$\lambda$${% endraw %} 为特征值，$$v$$ 为特征向量。
  - 矩阵形式为 {% raw %}$$A=V\Lambda V^{-1}$${% endraw %}，其中 $$V$$ 由特征向量组成，$$\Lambda$$ 为对角矩阵，特征值在主对角线上。

- 应用：

  - 数据分析（找到主要变化方向）
  - 可视化（降到 2D/3D）
  - 数据预处理

```python
from sklearn.decomposition import PCA
```

#### **奇异值分解（SVD, Singular Value Decomposition）**

- 任意数据矩阵 {% raw %}$$X \in \mathbb{R}^{m\times n}$${% endraw %} 可以写成：{% raw %}$$X=U\Sigma V^\top$${% endraw %}
  - $$U$$：左奇异向量（与样本相关），来自 {% raw %}$$XX^\top$${% endraw %} 的特征向量
  - $$\Sigma$$：奇异值（数值大小表示信息量），对角线为非负实数
  - $$V$$：右奇异向量（与特征维度相关），来自 {% raw %}$$X^\top X$${% endraw %} 的特征向量

- 应用：
  - 信息检索（Latent Semantic Analysis，文本降维）
  - 图像压缩（保留前 $$k$$ 个奇异值）
  - 一般的低秩近似和噪声去除

- **与特征分解的关系**
  - 对于任意矩阵 $$X$$ 做 SVD：{% raw %}$$X=U\Sigma V^\top$${% endraw %}
  - 两边右乘 $$V$$，得到：{% raw %}$$XV=U\Sigma$${% endraw %}
  - 两边左乘 {% raw %}$$X^\top$${% endraw %}，得到：{% raw %}$$X^\top XV=V\Sigma^2$${% endraw %}，特征分解形式为 {% raw %}$$Av=\lambda v$${% endraw %}
  - 对号入座：{% raw %}$$A = X^\top X,\ \lambda=\Sigma^2,\ v= V$${% endraw %}
  - 因此 **PCA 实际上是对数据矩阵的 SVD 在统计意义上的应用**。

#### 线性判别分析（LDA, Linear Discriminant Analysis）

- 一种**有监督的线性降维方法**。

- 目标：找到一个低维子空间，使得样本在该子空间中 **最大化类间差异**、**最小化类内差异**（利用类别标签信息）。

- 过程：

  - 类内散度矩阵：{% raw %}$$S_W=\sum_{i=1}^c\sum_{x\in C_i}(x-\mu_i)(x-\mu_i)^\top$${% endraw %}
  - 类间散度矩阵：{% raw %}$$S_B=\sum_{i=1}^cN_i(\mu_i-\mu)(\mu_i-\mu)^\top$${% endraw %}，$$N_i$$ 为第 $$i$$ 类样本数
  - 找到投影矩阵 $$W$$，使得 {% raw %}$$\max_W \frac{|W^TS_BW|}{|W^TS_WW|}$${% endraw %}，这是一个广义特征值分解问题。

- <span style="color:green">ps：如果类别数为 $$c$$，最多能降到 $$c-1$$ 维。</span>

- 应用：
  - 分类任务前用 LDA 降维，增强可分性，提高分类器性能
  - 也可单独作为分类器：**Fisher 线性判别**

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA(n_components=2)  # 保留2维
X_lda = lda.fit_transform(X, y)  # X是特征，y是标签
```

#### 因子分析（FA, Factor Analysis）

- 核心思想：
  - 假设观测到的高维数据由**少数潜在因子**共同驱动，**目标是解释变量的相关结构**。
  - 观测变量向量 {% raw %}$$X=(X_1,X_2,...,X_p)^\top$${% endraw %}，模型：{% raw %}$$X=\mu+\Lambda F+\epsilon$${% endraw %}
    - $$\mu$$：均值向量
    - {% raw %}$$F=(F_1,\dots,F_m)^\top$$（$m\ll p$）{% endraw %}
    - $$\Lambda$$：{% raw %}$$(p\times m)$${% endraw %} 因子载荷矩阵
    - {% raw %}$$\epsilon=(\epsilon_1,\dots,\epsilon_p)^\top$${% endraw %}：独特因子

- 步骤
  - **相关矩阵分析** → **因子提取**（最大似然/主因子法；特征值 > 1 或碎石图） → **因子旋转**（Varimax/Promax） → **因子解释**。
  - 因子模型 {% raw %}$$y-\mu=Lf+\epsilon$${% endraw %}，若 $$T$$ 为任意正交矩阵，则 {% raw %}$$L^*=LT,\ f^*=T^\top f$${% endraw %}，说明载荷与因子可旋转以获得更可解释解。

#### 独立成分分析（ICA, Independent Component Analysis）

- 假设数据是**若干相互独立的非高斯源信号**经**线性混合**得到；目标是找到使成分尽量独立的线性变换。
- 应用：盲源分离、信号去噪、特征提取（图像/语音）。
- 常见算法：FastICA、Infomax、JADE/SOBI。

#### comparison

- PCA 最大化方差，不考虑独立性；ICA 假设独立，从混合中分离独立成分。
- 因子分析关注共同性（公共因子+误差），与 ICA 的独立性假设不同。

### 非线性降维方法

优化损失函数实现数据降维，而不是采用线性映射的方式。

#### 多维尺度分析（MDS, Multidimensional Scaling）

- 基于距离度量的数据降维方法，要求将高维数据转换为低维后，样本点间相对位置关系不变。

- 目标：

  {% raw %}
  $$
  \min\ \sum_{i<j}\big(\lVert z_i-z_j\rVert - d_{ij}\big)^2
  $$
  {% endraw %}

  其中 $$\lVert z_i-z_j\rVert$$ 为低维欧式距离，$$d_{ij}$$ 为高维距离/不相似度。

- 一般步骤：计算距离矩阵 $$D$$ → 双中心化得到 $$B$$ → 对 $$B$$ 做特征分解 → 取前几大特征值/向量得 $$Z$$。

#### Isomap 等距特征映射

- 是 MDS 在流形结构上的应用：用**测地线距离**替代欧氏距离，再让低维嵌入保留这些距离。

#### 局部线性嵌入（LLE, Locally Linear Embedding）

- 核心：每个样本用邻居的线性组合近似，先在高维求权重 $$w_{ij}$$，再在低维保持同样的 $$w_{ij}$$。

  {% raw %}
  $$
  \min\ \sum_i\left\lVert x^i-\sum_j w_{ij}x^j\right\rVert^2,\quad
  \min\ \sum_i\left\lVert z^i-\sum_j w_{ij}z^j\right\rVert^2
  $$
  {% endraw %}

- 邻居数 $$K$$ 为超参数。

#### t-SNE (t-distributed Stochastic Neighbor Embedding)

- SNE 的改进版：在高维与低维分别构造概率分布，最小化 KL 散度。

|      | SNE                                                          | t-SNE                                                        |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 高维 | {% raw %}$$p_{j\mid i}=\dfrac{\exp(-\lVert x_i-x_j\rVert^2/2\sigma_i^2)}{\sum_{k\ne i}\exp(-\lVert x_i-x_k\rVert^2/2\sigma_i^2)}$${% endraw %} | {% raw %}$$p_{ij}=\dfrac{p_{j\mid i}+p_{i\mid j}}{2n}$${% endraw %} |
| 低维 | {% raw %}$$q_{j\mid i}=\dfrac{\exp(-\lVert y_i-y_j\rVert^2)}{\sum_{k\ne i}\exp(-\lVert y_i-y_k\rVert^2)}$${% endraw %} | {% raw %}$$q_{ij}=\dfrac{(1+\lVert y_i-y_j\rVert^2)^{-1}}{\sum_{k\ne l}(1+\lVert y_k-y_l\rVert^2)^{-1}}$${% endraw %} |
| 目标 | {% raw %}$$\sum_i KL(P_i\Vert Q_i)$${% endraw %}               | {% raw %}$$KL(P\Vert Q)=\sum_{i\ne j}p_{ij}\log\dfrac{p_{ij}}{q_{ij}}$${% endraw %} |
| 备注 | 梯度消失、拥挤问题                                           | 复杂度高（$$O(N^2)$$），参数敏感（perplexity 等）              |

```python
from sklearn.manifold import TSNE
Y = TSNE(n_components=2, perplexity=30, learning_rate='auto', n_iter=1000,
         init='pca', early_exaggeration=12.0, verbose=1).fit_transform(X)
```

#### UMAP (Uniform Manifold Approximation and Projection)

- 将样本构成图，并最小化高维/低维图的交叉熵差异。

- 高维图：为每点找 $$k\approx$$ `n_neighbors` 个最近邻，边权

  {% raw %}
  $$
  w_{ij} = \exp\!\Big(-\frac{d(x_i,x_j)-\rho_i}{\sigma_i}\Big)
  $$
  {% endraw %}

- 低维图：随机初始化后，通过 **交叉熵 + SGD** 优化。

```python
import umap
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
emb = reducer.fit_transform(data)  # -> (N,2)
```

| parameter    | remark             | value examples                                     |
| ------------ | ------------------ | -------------------------------------------------- |
| n_neighbors  | 高维邻居数         | 小：5–15 强局部；大：50–200 强全局                 |
| min_dist     | 低维点间的最小距离 | 小：簇紧凑；大：更分散                             |
| n_components | 目标维度           | 2 或 3                                             |
| metric       | 距离度量           | euclidean, manhattan, cosine, correlation, hamming |

### 特征选择方法

#### Filter / Wrapper / Embedded（略）

### 基于深度学习的方法（略）

### 预先知识

{% raw %}
$$
\text{Var}(X)=\mathbb{E}[(X-\mathbb{E}[X])^2]
$$
{% endraw %}

{% raw %}
$$
\begin{aligned}
\text{Cov}(X,Y) &= \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] \\
&= \mathbb{E}(XY)-\mathbb{E}(X)\,\mathbb{E}(Y)
\end{aligned}
$$
{% endraw %}

### 个人理解

- 矩阵乘法

  {% raw %}
  $$
  C_{m\times n}=X_{m\times k}Y_{k\times n},\quad
  c_{ij}=\sum_{z=1}^{k} x_{i,z}y_{z,j}
  $$
  {% endraw %}

  在机器学习中，数据矩阵常为 $$m\times n$$（$$m$$ 个样本，$$n$$ 维特征）。当降维时，为减少特征维度，**通常右乘降维矩阵**以控制输出的特征维数。左乘矩阵可视为对样本维度的线性组合（如加权）。

- 注意 <span style="color:red">**协方差矩阵（总体） vs. 样本协方差矩阵**</span>：样本协方差矩阵

  {% raw %}
  $$
  S=\frac{1}{m-1}\sum_{k=1}^{m}(x_i^{(k)}-\tilde x_i)(x_j^{(k)}-\tilde x_j)
  $$
  {% endraw %}

  其中 $$m-1$$ 为无偏估计（Bessel 校正）。

- 中心化（center）：对每一列（特征）减去均值，使均值为 0。  

- 白化（whiten）：把每个主成分方向上的方差标准化为 1。  

- 载荷矩阵（loadings / components）：PCA 的特征方向矩阵 $$V_k$$ 的转置。
